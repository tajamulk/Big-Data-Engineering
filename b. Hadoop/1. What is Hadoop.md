## Hadoop
![alt text](Images/hadoop.png)
is an open source tool designed to handle massive amounts of data in distributed and scalable way.

It can handle vast amounts of structured, semi-structured and unstructured data.

### Key Components

#### 1. Hadoop Distributed File System (HDFS): 
A distributed file storage system that splits large data files into smaller blocks and distributes them across a cluster of machines.
Ensures fault tolerance by replicating data blocks across multiple nodes.

#### 2. MapReduce: 
A programming model for distributed data processing.
Consists of two stages:
- Map: Processes input data and converts it into key-value pairs.
- Reduce: Aggregates the results from the Map phase to produce the final output.

#### 3. YARN (Yet Another Resource Negotiator):
Manages resources and applications run schedule in a Hadoop cluster.

#### 4. Hadoop Common
Provides the utilities and libraries needed for the other Hadoop components to operate.

### Advantages of Hadoop
- Scalability: Easily scales from a single server to thousands of machines.
- Fault Tolerance: Automatically replicates data to ensure reliability in case of hardware failures.
- Cost-Effective: Uses commodity hardware, making it affordable for large-scale data processing.
- Flexibility: Handles various data formats (structured, semi-structured, unstructured).

### Use Cases
- Managing large data for businesses
- Analyzing customer behaviour 
- Training machine learning models.

