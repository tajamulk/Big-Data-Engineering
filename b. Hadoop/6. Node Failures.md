## Node Failures

### Data Node Failures
situations where one or more DataNodes become unavailable or crash leading to unavailability of data

1. Temporary Data Node Failure
2. Permanent Data Node Failure

| **Aspect**               | **Temporary DataNode Failure**                                    | **Permanent DataNode Failure**                                |
|--------------------------|-------------------------------------------------------------------|--------------------------------------------------------------|
| **Definition**           | Short-term unavailability of the DataNode.                      | Irrecoverable loss of the DataNode.                          |
| **Causes**               | Network issues, reboot, minor software/hardware glitches.       | Disk crash, hardware failure, or intentional decommissioning.|
| **Duration**             | Short-term, usually recoverable.                                | Long-term or indefinite.                                     |
| **Impact on Data**       | Minimal; data is intact and accessible via replicas.            | Blocks on the failed node must be replicated elsewhere.      |
| **NameNode Response**    | Marks DataNode as unavailable; replication may start if prolonged. | Removes DataNode from the cluster and initiates replication. |
| **Process**              | 1. DataNode stops sending heartbeats. <br> 2. NameNode waits for the timeout period. <br> 3. Replication starts if needed. <br> 4. DataNode reconnects and syncs. | 1. DataNode stops sending heartbeats. <br> 2. NameNode marks it as dead. <br> 3. Replication immediately starts for affected blocks. <br> 4. Cluster operates without the failed node. |
| **Recovery**             | Node rejoins cluster once the issue is resolved.                | Blocks are re-replicated to other nodes; node replacement required. |
| **Examples**             | Temporary network outage or system maintenance.                 | Disk failure or physical damage to the DataNode.             |
| **Prevention**           | Regular monitoring and quick resolution of transient issues.    | Maintain proper replication factor and replace failed nodes promptly. |

#### Key Takeaways:
1. HDFS handles failures gracefully.
2. Metadata is the backbone of HDFS.
3. RF is strictly maintained.


### How are data node failures handled by hadoop ?
How HDFS Handles DataNode Failures:

Replication: Each data block is replicated (default replication factor = 3) across multiple DataNodes. If a DataNode fails, the block's replica on another DataNode can still be accessed.
Re-replication: The NameNode detects the failure and triggers re-replication of the lost data blocks to healthy DataNodes.
Automatic Recovery: The system automatically recovers from DataNode failures by using the replicated copies and maintaining the integrity of the data.
Key Points:
Data redundancy through replication ensures that data is not lost.
Fault tolerance allows the system to recover from failures and continue operations without data loss.


